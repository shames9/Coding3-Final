{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Realtime-emotion-detectionusing-videotest\n",
        "I hope that by analysing facial expressions, we can better understand and respond to people's emotional states. The code provides a simple and practical base framework to help developers quickly build and deploy applications for facial emotion analysis. By combining computer vision and deep learning techniques, we can create a more intelligent and human interaction experience.\n",
        "\n",
        "This code is a deep learning based facial emotion analysis application. It uses computer vision and machine learning techniques to detect faces captured by the camera and to predict the emotion category of facial expressions. The code performs face detection using OpenCV by loading pre-trained models and weights, and pre-processes and classifies the detected face regions for emotion.\n",
        "\n",
        "## Reference\n",
        "[CSDN](https://blog.csdn.net/weixin_42143481/article/details/105771183?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168681998016800225516119%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168681998016800225516119&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-105771183-null-null.142^v88^insert_down38v5,239^v2^insert_chatgpt&utm_term=colab%E6%91%84%E5%83%8F%E5%A4%B4&spm=1018.2226.3001.4187)\n",
        "\n",
        "https://morioh.com/p/801c509dda99\n",
        "\n",
        "https://github.com/Dhanush45/Realtime-emotion-detectionusing-python"
      ],
      "metadata": {
        "id": "XYsz4YVnzK48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing necessary libraries for plotting and image processing"
      ],
      "metadata": {
        "id": "PSB_eSco8vII"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkVmoC3ie5EK"
      },
      "outputs": [],
      "source": [
        "import os                       #For interaction with the operating system\n",
        "import cv2                       #OpenCV library for image processing\n",
        "import numpy as np                  #For numerical calculations\n",
        "from keras.models import model_from_json       #Functions for loading models from JSON files in Keras\n",
        "from keras.preprocessing import image        #Image pre-processing tools are provided\n",
        "\n",
        "from IPython.display import display, Javascript   #For displaying images or other contentï¼ŒAllow JavaScript code to be executed\n",
        "from google.colab.output import eval_js       #For evaluating JavaScript code\n",
        "from base64 import b64decode             #For decoding Base64 encoded data\n",
        "\n",
        "from IPython.display import HTML, Audio       #Module for displaying HTML content and playing audio\n",
        "\n",
        "import matplotlib.pyplot as plt           #For creating charts\n",
        "from google.colab.patches import cv2_imshow     #Function from the google.colab.patches module for displaying images in Google Colab.\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode             #For decoding Base64 encoded data\n",
        "import numpy as np\n",
        "import io                       #For input/output operations\n",
        "from PIL import Image\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For capturing video frames in web pages and converting them to images"
      ],
      "metadata": {
        "id": "QE3-CGLW8XcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VIDEO_HTML = \"\"\"\n",
        "<video autoplay\n",
        " width=%d height=%d style='cursor: pointer;'></video>\n",
        "<script>\n",
        "\n",
        "var video = document.querySelector('video')\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({ video: true })\n",
        "  .then(stream=> video.srcObject = stream)\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "  video.onclick = ()=>{\n",
        "    var canvas = document.createElement('canvas')\n",
        "    var [w,h] =[video.offsetWidth, video.offsetHeight]\n",
        "    canvas.width = w\n",
        "    canvas.height = h\n",
        "    canvas.getContext('2d')\n",
        "          .drawImage(video, 0, 0, w, h)\n",
        "    video.srcObject.getVideoTracks()[0].stop()\n",
        "    video.replaceWith(canvas)\n",
        "    resolve(canvas.toDataURL('image/jpeg', %f))\n",
        "  }\n",
        "})\n",
        "</script>\n",
        "\"\"\"\n",
        "#A string variable containing HTML and JavaScript is defined to display the video in the web page.\n",
        "#Get the video element in a web page.\n",
        "#Use the navigator.mediaDevices.getUserMedia method to request user authorization to access the video device and assign the video stream to the srcObject property of the video element to play the video in the web page\n",
        "#Create a Promise object to be triggered when the user clicks on the video. In the click event handler, create a canvas element to draw the video frame. Get the width and height of the video element and set the size of the canvas. Draw the video frame to the canvas using the drawImage method of the 2D context. Stop the video stream and replace the video element with the canvas. Finally, convert the canvas to Base64 encoding of the image and use the result as a parsed value for the Promise.\n",
        "#The purpose of the entire code block is to capture video frames and convert them into image data for subsequent processing or display."
      ],
      "metadata": {
        "id": "s4DZLkn3e-Bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Capture a photo from a video and return its image data"
      ],
      "metadata": {
        "id": "eT1ivR8s9UVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def take_photo(filename='photo.jpg', quality=0.8, size=(800,600)):  #A function called take_photo is defined which accepts as arguments the filename, image quality and size, with default values of 'photo.jpg', 0.8 and (800,600) respectively.\n",
        "  display(HTML(VIDEO_HTML % (size[0],size[1],quality)))       #Display the HTML content in a Jupyter/Colab notebook, using the previously defined string VIDEO_HTML and inserting the values of size and quality into the HTML.\n",
        "  data = eval_js(\"data\")                       #Get the value of the data variable defined earlier by executing the JavaScript code eval_js(\"data\").\n",
        "  binary = b64decode(data.split(',')[1])               #Separate the values of the data variable by commas and decode the Base64 encoded image data.\n",
        "  f = io.BytesIO(binary)                       #Store the decoded image data in the io.BytesIO object.\n",
        "  return np.asarray(Image.open(f))                  #Open the image in f and return it after converting it to a NumPy array.\n",
        "                                     #The function does this by displaying the video stream and waiting for the user to click to capture a photo, then converting the photo to a NumPy array to be returned for further processing or analysis.\n"
      ],
      "metadata": {
        "id": "N_V2V2OzfEbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load model\n",
        "model = model_from_json(open(\"fer.json\", \"r\").read())\n",
        "#load weights\n",
        "model.load_weights('fer.h5')"
      ],
      "metadata": {
        "id": "B4Hy9nzOfI-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "face_haar_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "#A cascade classifier file called 'haarcascade_frontalface_default.xml' is loaded. A cascade classifier is an algorithm used for object detection and this file is specifically used for detecting faces. face_haar_cascade variable will hold this cascade classifier object and can be used to detect faces in images or videos."
      ],
      "metadata": {
        "id": "8yJ_s6r4f3oE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cap=cv2.VideoCapture(0)                              #Open the default camera device and create a VideoCapture object and assign it to the cap variable.\n",
        "\n",
        "while True:                                    #Enter an infinite loop for continuous processing of video frames\n",
        "    test_img=take_photo()                           # captures frame and returns boolean value and captured image\n",
        "\n",
        "    gray_img= cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)           #Converts the captured colour image into a greyscale image for subsequent face detection.\n",
        "\n",
        "\n",
        "\n",
        "    faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5) #Use the face_haar_cascade cascade classifier object to detect faces in a grey-scale image, returning the coordinates of the rectangular region of the face.\n",
        "\n",
        "\n",
        "    for (x,y,w,h) in faces_detected:\n",
        "        cv2.rectangle(test_img,(x,y),(x+w,y+h),(255,0,0),thickness=7)\n",
        "        roi_gray=gray_img[y:y+w,x:x+h]                   #cropping region of interest i.e. face area from  image\n",
        "        roi_gray=cv2.resize(roi_gray,(48,48))\n",
        "\n",
        "        img_pixels = roi_gray.astype('float32')\n",
        "        img_pixels = np.expand_dims(img_pixels, axis=0)\n",
        "        img_pixels /= 255.0\n",
        "                                        #For each face detected, a blue rectangular box is drawn and the region of interest (ROI), the face region, is truncated. The ROI is then resized to 48x48 and the data is normalised.\n",
        "\n",
        "        predictions = model.predict(img_pixels)             #Emotion prediction on normalised face images using a pre-trained model mod.\n",
        "\n",
        "        #find max indexed array\n",
        "        max_index = np.argmax(predictions[0])\n",
        "\n",
        "        emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
        "        predicted_emotion = emotions[max_index]\n",
        "                                        #Find the sentiment category with the highest probability in the predicted outcome and convert it to the corresponding sentiment label.\n",
        "        cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2) #Plotting the predicted sentiment labels on the image.\n",
        "\n",
        "    resized_img = cv2.resize(test_img, (1000, 700))\n",
        "                                        #cv2.imshow('Facial emotion analysis ',resized_img)\n",
        "    plt.imshow(cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB))\n",
        "                                        # as opencv loads in BGR format by default, we want to show it in RGB.\n",
        "    plt.show()\n",
        "\n",
        "    if cv2.waitKey(10) == ord('q'):                   #wait until 'q' key is pressed\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows\n",
        "#Free up camera resources and close all windows."
      ],
      "metadata": {
        "id": "T6DYYgg6gKtl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}